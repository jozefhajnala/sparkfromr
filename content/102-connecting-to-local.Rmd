# Connecting and using a local Spark instance

The following code chunks will prepare the R session for us to be able to experiment with the code presented in the book. We will attach the needed R packages, initialize and connect to a local Spark instance and copy the weather and flights datasets from the nycflights13 package to the Spark instance such that we can work with them in our examples.


## Packages and data

We will be using the sparklyr package to interface with Spark and since this packages works very well with the dplyr package and its vocabulary, we will take advantage of the dplyr syntax and the pipe operator. As a source of data, we will use the nycflights13 package that conveniently provides airline data for flights departing New York City in 2013. 

```{r r202_preparation}
# Attach packages
suppressPackageStartupMessages({
  library(sparklyr)
  library(dplyr)
  library(nycflights13)
})

# Add an id column to the datasets
weather <- nycflights13::weather %>%
  mutate(id = 1L:nrow(nycflights13::weather)) %>% 
  select(id, everything())

flights <- nycflights13::flights %>%
  mutate(id = 1L:nrow(nycflights13::flights)) %>% 
  select(id, everything())
```

## Connecting to Spark and providing it with data

As a second step, we will now connect to a Spark instance which will be running on our local machine and send the prepared data to the instance using the `copy_to()` function such that we can work with them in Spark.

We assign the outputs of `copy_to()` to objects called `tbl_weather` and `tbl_flights`, which are references to the DataFrame objects within Spark.

```{r 202_connection}
# Connect to a local Spark instance
sc <- sparklyr::spark_connect(master = "local")

# Copy the weather dataset to the instance
tbl_weather <- dplyr::copy_to(
  dest = sc, 
  df = weather,
  name = "weather",
  overwrite = TRUE
)

# Copy the flights dataset to the instance
tbl_flights <- dplyr::copy_to(
  dest = sc, 
  df = flights,
  name = "flights",
  overwrite = TRUE
)
```

## First glance at the data

To make sure our datasets are available in the Spark instance, we can look at the first few rows of the two datasets we have copied to Spark. Notice how the print shows `Source: spark<?> [?? x`, telling us that the data inded comes from a Spark instance and the data frames have an unknown amount of rows. This is because Spark will do minimal work to show us just the first 6 rows, instead of going though the entire dataset.

```{r 202_glance}
head(tbl_flights)
head(tbl_weather)
```