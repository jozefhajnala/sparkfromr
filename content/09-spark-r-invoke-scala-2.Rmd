# Exploring the invoke API from R with Java reflection and examining invokes with logs

```{block, type='wizardry'}
Then darkness took me, and I strayed out of thought and time, and I wandered far on roads that I will not tell

* Gandalf the White
```

In the previous chapters, we have shown how to write functions as both [combinations of dplyr verbs](https://jozef.io/r202-spark-r-dplyr-verbs/), [SQL query generators](https://jozef.io/r203-spark-r-sql/) that can be executed by Spark and [how to use the lower-level API](https://jozef.io/r204-spark-r-invoke-scala/) to invoke methods on Java object references from R.

In this chapter, we will look into more details around sparklyr's `invoke()` API, investigate available methods for different classes of objects using the Java reflection API and look under the hood of the sparklyr interface mechanism with invoke logging.


## Examining available methods from R

If you did not do so, it is recommended to read the [previous chapter](using-the-lower-level-invoke-api-to-manipulate-sparks-java-objects-from-r.html) of this book before this one to get a quick overview of the `invoke()` API.


## Using the Java reflection API to list the available methods

The `invoke()` interface is powerful, but also a bit hidden from the eyes as we do not immediately know what methods are available for which object classes. We can circumvent that using the `getMethods` method which (in short) returns an array of Method objects reflecting public member methods of the class. 

For instance, retrieving a list of methods for the `org.apache.spark.SparkContext` class:

```{r r205_invoke_getmethods}
mthds <- sc %>% spark_context() %>%
  invoke("getClass") %>%
  invoke("getMethods")
head(mthds)
```

We can see that the `invoke()` chain has returned a list of Java object references, each of them of class `java.lang.reflect.Method`. This is a good result, but the output is not very user-friendly from the R user perspective. Let us write a small wrapper that will return a some of the method's details in a more readable fashion, for instance the return type and an overview of parameters:

```{r r205_define_getMethodDetails}
getMethodDetails <- function(mthd) {
  returnType <- mthd %>% invoke("getReturnType") %>% invoke("toString")
  params <- mthd %>% invoke("getParameters")
  params <- vapply(params, invoke, "toString", FUN.VALUE = character(1))
  c(returnType = returnType, params = paste(params, collapse = ", "))
}
```

Finally, to get a nice overview, we can make another helper function that will return a named list of methods for an object's class, including their return types and overview of parameters:

```{r r205_define_getAvailableMethods}
getAvailableMethods <- function(jobj) {
  mthds <- jobj %>% invoke("getClass") %>% invoke("getMethods")
  nms <- vapply(mthds, invoke, "getName", FUN.VALUE = character(1))
  res <- lapply(mthds, getMethodDetails)
  names(res) <- nms
  res
}
```


## Investigating DataSet and SparkContext class methods

Using the above defined function we can explore the methods available to a DataFrame reference, showing a few of the names first:

```{r r205_preview_getAvailableMethods}
dfMethods <- tbl_flights %>% spark_dataframe() %>%
  getAvailableMethods()

# Show some method names:
dfMethodNames <- sort(unique(names(dfMethods)))
head(dfMethodNames, 20)
```

If we would like to see more details we can now investigate further, for instance show different parameter interfaces for the `agg` method, showing that the `agg` method has the following parameter interfaces:

```{r r205_investigate_methods_df}
sort(vapply(
  dfMethods[names(dfMethods) == "agg"], 
  `[[`, "params",
  FUN.VALUE = character(1)
))
```

Similarly, we can look at a `SparkContext` class and show some available methods that can be invoked:

```{r r205_investigate_methods_sc}
scMethods <- sc %>% spark_context() %>%
  getAvailableMethods()
scMethodNames <- sort(unique(names(scMethods)))
head(scMethodNames, 60)
```

### Using helpers to explore the methods

We can also use the helper functions to investigate more. For instance, we see that there is a `getConf` method avaiable to us. Looking at the object reference however does not provide useful information, so we can list the methods for that class and look for `"get"` methods that would show us the configuration:

```{r r205_investigate_methods_conf}
spark_conf <- sc %>% spark_context() %>% invoke("conf")
spark_conf_methods <- spark_conf %>% getAvailableMethods() 
spark_conf_get_methods <- spark_conf_methods %>%
  names() %>%
  grep(pattern = "get", ., value = TRUE) %>%
  sort()
spark_conf_get_methods
```

We see that there is a `getAll` method that could prove useful, returning a list of tuples and taking no arguments as input:

```{r r205_investigate_methods_conf_get}
# Returns a list of tuples, takes no arguments:
spark_conf_methods[["getAll"]]

# Invoke the `getAll` method and look at part of the result
spark_confs <- spark_conf %>% invoke("getAll")
spark_confs <- vapply(spark_confs, invoke, "toString", FUN.VALUE = character(1))
sort(spark_confs)[c(2, 3, 12, 14)]
```

Looking at [the Scala documentation for the `getAll` method](https://spark.apache.org/docs/1.6.0/api/java/org/apache/spark/SparkConf.html#getAll()), we actually see that there is information missing on our data - the classes of the objects in the tuple, which in this case is `scala.Tuple2<java.lang.String,java.lang.String>[]`.

We could therefore improve our helper to be more detailed in the return value information.

### Unexported helpers provided by sparklyr

The sparklyr package itself provides facilities of nature similar to those above, looking at some of them, even though they are not exported:

```{r r205_sparkly_helpers}
sparklyr:::jobj_class(spark_conf)
sparklyr:::jobj_info(spark_conf)$class
```
```{r r205_sparkly_helpers_capture, eval=FALSE}
capture.output(sparklyr:::jobj_inspect(spark_conf)) %>% head(10)
```
```{r r205_sparkly_helpers_output, echo=FALSE}
# Hardcode the output to prevent useless diffs when re-rendering
c("<jobj[1645]>", "  org.apache.spark.SparkConf", "  org.apache.spark.SparkConf@7ec389e7", 
"Fields:", "<jobj[2490]>", "  java.lang.reflect.Field", "  private final java.util.concurrent.ConcurrentHashMap org.apache.spark.SparkConf.org$apache$spark$SparkConf$$settings", 
"<jobj[2491]>", "  java.lang.reflect.Field", "  private transient org.apache.spark.internal.config.ConfigReader org.apache.spark.SparkConf.org$apache$spark$SparkConf$$reader"
)
```


## How sparklyr communicates with Spark, invoke logging

Now that we have and overview of the `invoke()` interface, we can take a look under the hood of sparklyr and see how it actually communicates with the Spark instance. In fact, the communication is a set of invocations that can be very different depending on which of the approches we choose for our purposes.

To obtain the information, we use the `sparklyr.log.invoke` property. We can choose one of the following 3 values based on our preferences:

- `TRUE` will use `message()` to communicate short info on what is being invoked
- `"cat"` will use `cat()` to communicate short info on what is being invoked
- `"callstack"` will use `message()` to communicate short info on what is being invoked and the callstack

We will use `TRUE` in our article to keep the output short and easily manageable. First, we will close the previous connection and create a new one with the configuration containing the `sparklyr.log.invoke` set to `TRUE`, and copy in the flights dataset:

```{r r205_invoke_logging_setup}
sparklyr::spark_disconnect(sc)
config <- sparklyr::spark_config()
config$sparklyr.log.invoke <- TRUE
suppressMessages({
  sc <- sparklyr::spark_connect(master = "local", config = config)
  tbl_flights <- dplyr::copy_to(sc, nycflights13::flights, "flights")
})
```

### Using dplyr verbs translated with dbplyr

Now that the setup is complete, we use the dplyr verb approach to retrieve the count of rows and look the invocations that this entails:

```{r r205_invoke_logging_dplyr}
tbl_flights %>% dplyr::count()
```

We see multiple invocations do the `sql` method and also the `columns` method. This makes sense since the dplyr verb approach actually works by translating the commands into Spark SQL via dbplyr and then sends those translated commands to Spark via that interface.

### Using DBI to send queries

Similarly, we can investigate the invocations that happen when we try to retrieve the same results via the DBI interface:

```{r r205_invoke_logging_dbi}
DBI::dbGetQuery(sc, "SELECT count(1) AS n FROM flights")
```

We see slightly fewer invocations compared to the above dplyr approach, but the output is also less processed.

### Using the invoke interface

Looking at the invocations that get executed using the `invoke()` interface:

```{r r205_invoke_logging_invoke}
tbl_flights %>% spark_dataframe() %>% invoke("count")
```

We see that the amount of invocations is much lower, where the top 3 invocations come from the first part of the pipe. The `invoke("count")` part translated to exactly one invocation to the `count` method. We see therefore that the `invoke()` interface is indeed a more lower-level interface that invokes methods as we request them, with little to none overhead related to translations and other effects.

### Redirecting the invoke logs

When running R applications that use Spark as a calculation engine, it is useful to get detailed invoke logs for debugging and diagnostic purposes. Implementing such mechanisms, we need to take into consideration how R handles the invoke logs produced by sparklyr. In simple terms, the invoke logs produced when using 

- `TRUE` and `"callstack"` are created using `message()`, which means they get sent to the `stderr()` connection by default
- `"cat"` are created using `cat()`, so they get sent to `stdout()` connection by default

This info can prove useful when redirecting the log information from standard output and standard error to different logging targets.


## Conclusion

In this chapter, we have looked at using the Java reflection API with sparklyr's `invoke()` interface to get useful insight on available methods for different object types that can be used in the context of Spark, but also in other contexts. Using invoke logging, we have also shown how the different sparklyr interfacing methods communicate with Spark under the hood.
