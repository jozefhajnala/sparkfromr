# Constructing functions by piping dplyr verbs

<script src="static/js/highcharts.js"></script>
<script src="static/js/highcharts-more.js"></script>

In the [previous chapters](communication-between-spark-and-sparklyr.html), we looked at how the sparklyr interface communicates with the Spark instance and what this means for performance with regards to arbitrarily defined R functions. We also examined how Apache Arrow can increase the performance of data transfers between the R session and the Spark instance.

In this chapter, we will look at how to write R functions that can be executed directly by Spark without serialization overhead that we have shown in the previous installment. We will focus on writing functions as combinations of <span class="rpackage">dplyr</span> verbs that can be translated using dbplyr and investigate how the SQL is generated and Spark plans created.

## R functions as combinations of dplyr verbs and Spark

One of the approaches to retain the performance of Spark with arbitrary R functionality is to carefully design our functions such that in its entirety when using it with sparklyr, the function call can be translated directly to Spark SQL using dbplyr. 

This allows us to write, package, test, and document the functions as we normally would, while still getting the performance benefits of Apache Spark.

Let's look at an example where we would like to do simple transformations of data stored in a column of a data frame, such as normalization of one of the columns. For illustration purposes, we will normalize the values of a column by first subtracting the mean value and then dividing the values by the standard deviation.

### Trying it with base R functions

The first attempt could be quite simple, we could attempt to take advantage of R's base function `scale()` to do the work for us:

```{r r202_define_normalize_dplyr_scale}
normalize_dplyr_scale <- function(df, col, newColName) {
  df %>% mutate(!!newColName := scale({{col}}))
}
```

This function would work fine with a local data frame such as `weather`:

```{r r202_test_normalize_dplyr_scale_local}
weather %>%
  normalize_dplyr_scale(temp, "normTemp") %>%
  select(id, temp, normTemp)
```

However for a Spark DataFrame this would throw an error. This is because the base R function `scale()` is not translated by dbplyr at the moment and it is not a Hive built-in function either:

```{r r202_test_normalize_dplyr_scale_spark, error=TRUE, error.lines=4}
tbl_weather %>%
  normalize_dplyr_scale(temp, "normTemp") %>%
  select(id, temp, normTemp)
```

### Using a combination of supported dplyr verbs and operations

To run the function successfully, we will need to rewrite it as a combination of functions and operations that are supported by the dbplyr translation to Spark SQL. One example implementation is as follows:

```{r r202_define_normalize_dplyr}
normalize_dplyr <- function(df, col, newColName) {
  df %>% mutate(
    !!newColName := ({{col}} - mean({{col}}, na.rm = TRUE)) /
        sd({{col}}, na.rm = TRUE)
  )
}
```

Using this function yields the desired results for both local and Spark data frames:

```{r r202_test_normalize_dplyr}
## Local data frame
weather %>%
  normalize_dplyr(temp, "normTemp") %>%
  select(id, temp, normTemp)

## Spark DataFrame
tbl_weather %>%
  normalize_dplyr(temp, "normTemp") %>%
  select(id, temp, normTemp) %>% 
  collect()
```

### Investigating the SQL translation and its Spark plan

Another advantage of this approach is that we can investigate the plan by which the actions will be executed by Spark using the `explain()` function from the <span class="rpackage">dplyr</span> package. This will print both the SQL query constructed by <span class="rpackage">dbplyr</span> and the plan generated by Spark, which can help us investigate performance issues:

```{r r202_explain_normalize_dplyr}
tbl_weather %>%
  normalize_dplyr(temp, "normTemp") %>%
  dplyr::explain()
```

If we are only interested in the SQL itself as a character string, we can use <span class="rpackage">dbplyr</span>'s `sql_render()`:

```{r r202_sql_render_normalize_dplyr}
tbl_weather %>%
  normalize_dplyr(temp, "normTemp") %>%
  dbplyr::sql_render() %>%
  unclass()
```


## A more complex use case - Joins, group bys, and aggregations

The dplyr syntax makes it very easy to construct more complex aggregations across multiple Spark DataFrames. An example of a function that joins 2 Spark DataFrames and computes a mean of a selected column, grouped by another column can look as follows:

```{r r202_define_joingrpagg_dplyr}
joingrpagg_dplyr <- function(
  df1, df2, 
  joinColNames = setdiff(intersect(colnames(df1), colnames(df2)), "id"),
  col, groupCol
) {
  df1 %>%
    right_join(df2, by = joinColNames) %>%
    filter(!is.na({{col}})) %>% 
    group_by({{groupCol}}) %>%
    summarise(mean({{col}})) %>% 
    arrange({{groupCol}})
}
```

We can then use this function for instance to look at the mean arrival delay of flights grouped by visibility. Note that we are only collecting heavily aggregated data - 20 rows in total. The overhead of data transfer from the Spark instance to the R session is therefore small. Also, just assigning the function call to `delay_by_visib` does not actually execute or collect anything, execution really starts only when `collect()` is called:

```{r r202_use_joingrpagg_dplyr}
delay_by_visib <- joingrpagg_dplyr(
  tbl_flights, tbl_weather,
  col = arr_delay, groupCol = visib
)
delay_by_visib %>% collect()
```

We can look at the plan and the generated SQL query as well:

```{r r202_explain_joingrpagg_dplyr}
delay_by_visib %>% dplyr::explain()
```

## Using the functions with local versus remote datasets

### Unified front-end, different back-ends

Some of the appeal of the <span class="rpackage">dplyr</span> syntax comes from the fact that we can use the same functions to conveniently manipulate local data frames in memory and, with the very same code, data from remote sources such as relational databases, data.tables and even data within Spark.

This unified front-end, however, comes with some important differences that we must be aware of when applying and porting code from using it to manipulate and compute on local data versus on remote sources. The same holds for remote Spark DataFrames that we are manipulating when using <span class="rpackage">dplyr</span> functions.

The following paragraphs will show a few examples of issues we can come across when porting local data handling to a remote source such as Spark.

### Differences in `NA` and `NaN` values

Another example of differences can arise from handling `NA` and `NaN` values:

```{r r202_reconcile_join_na_counts}
bycols <- c("year", "month", "day", "origin", "hour")

# Create left joins
joined_spark <- tbl_flights %>%
  left_join(tbl_weather, by = bycols) %>%
  collect()
joined_local <- flights %>%
  left_join(weather, by = bycols)

# Look at counts of NaN values
joined_local %>% filter(is.nan(temp)) %>% count()
joined_spark %>% filter(is.nan(temp)) %>% count()
```

### Dates, times and time zones

Special care must also be taken when dealing with date/time values and their time zones:

```{r r202_reconcile_datetimes}
# Note the time_hour values are different
weather %>% select(id, time_hour)
tbl_weather %>% select(id, time_hour)
```

### Joins

Another example of a different behavior is joining due to matching column values. Care must be taken to write conditions that are portable to ensure that the joins are consistent across data sources.

### Portability of used methods

And, rather obviously, when using Hive built-in functions in the dplyr-based function, we will most likely not be able to execute it on the local data frames, as we have [seen previously](communication-between-spark-and-sparklyr.html#a-hive-built-in-function-not-existing-in-r).

## Conclusion, take-home messages

In this chapter, we have shown that we can take advantage of the performance of Spark while still writing arbitrary R functions by using dplyr syntax, which supports translation to Spark SQL using the <span class="rpackage">dbplyr</span> backend. We have also looked at some important differences when applying the same dplyr transformations to local and remote data sets.

With this approach, we can use R development best practices, testing, and documentation methods in a standard way when writing our R packages, getting the best of both worlds - Apache Spark for performance and R for convenient development of data science applications.

In the next chapter, we will look at writing R functions that will be using SQL directly, instead of relying on <span class="rpackage">dbplyr</span> for the translation, and how we can efficiently send them to the Spark instance for execution and optionally retrieve the results to our R session.
